{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display, HTML\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Fix the dying kernel problem (only a problem in some installations - you can remove it, if it works without it)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-charleston",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_ratings_df = pd.read_csv(os.path.join(\"data\", \"movielens_small\", \"ratings.csv\")).rename(columns={'userId': 'user_id', 'movieId': 'item_id'})\n",
    "ml_movies_df = pd.read_csv(os.path.join(\"data\", \"movielens_small\", \"movies.csv\")).rename(columns={'movieId': 'item_id'})\n",
    "ml_df = pd.merge(ml_ratings_df, ml_movies_df, on='item_id')\n",
    "ml_df.head(10)\n",
    "\n",
    "display(HTML(ml_movies_df.head(10).to_html()))\n",
    "\n",
    "# Filter the data to reduce the number of movies\n",
    "seed = 6789\n",
    "rng = np.random.RandomState(seed=seed)\n",
    "left_ids = rng.choice(ml_movies_df['item_id'], size=1000, replace=False)\n",
    "\n",
    "ml_ratings_df = ml_ratings_df.loc[ml_ratings_df['item_id'].isin(left_ids)]\n",
    "ml_movies_df = ml_movies_df.loc[ml_movies_df['item_id'].isin(left_ids)]\n",
    "ml_df = ml_df.loc[ml_df['item_id'].isin(left_ids)]\n",
    "\n",
    "print(\"Number of chosen interactions: {}\".format(len(ml_ratings_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-renaissance",
   "metadata": {},
   "source": [
    "# Recommender class\n",
    "\n",
    "Remark: Docstrings written in reStructuredText (reST) used by Sphinx to automatically generate code documentation. It is also used by default by PyCharm (type triple quotes after defining a class or a method and hit enter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender(object):\n",
    "    \"\"\"\n",
    "    Base recommender class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize base recommender params and variables.\n",
    "        \n",
    "        :param int seed: Seed for the random number generator.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, interactions_df, users_df, items_df):\n",
    "        \"\"\"\n",
    "        Training of the recommender.\n",
    "        \n",
    "        :param pd.DataFrame interactions_df: DataFrame with recorded interactions between users and items \n",
    "            defined by user_id, item_id and features of the interaction.\n",
    "        :param pd.DataFrame users_df: DataFrame with users and their features defined by user_id and the user feature columns.\n",
    "        :param pd.DataFrame items_df: DataFrame with items and their features defined by item_id and the item feature columns.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def recommend(self, users_df, items_df, n_recommendations=1):\n",
    "        \"\"\"\n",
    "        Serving of recommendations. Scores items in items_df for each user in users_df and returns \n",
    "        top n_recommendations for each user.\n",
    "        \n",
    "        :param pd.DataFrame users_df: DataFrame with users and their features for which recommendations should be generated.\n",
    "        :param pd.DataFrame items_df: DataFrame with items and their features which should be scored.\n",
    "        :param int n_recommendations: Number of recommendations to be returned for each user.\n",
    "        :return: DataFrame with user_id, item_id and score as columns returning n_recommendations top recommendations \n",
    "            for each user.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        recommendations = pd.DataFrame(columns=['user_id', 'item_id', 'score'])\n",
    "        \n",
    "        for ix, user in users_df.iterrows():\n",
    "            user_recommendations = pd.DataFrame({'user_id': user['user_id'],\n",
    "                                                 'item_id': [-1] * n_recommendations,\n",
    "                                                 'score': [3.0] * n_recommendations})\n",
    "\n",
    "            recommendations = pd.concat([recommendations, user_recommendations])\n",
    "\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-warehouse",
   "metadata": {},
   "source": [
    "# Evaluation measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-perspective",
   "metadata": {},
   "source": [
    "## Explicit feedback - ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-thumbnail",
   "metadata": {},
   "source": [
    "### MAE - Mean Absolute error\n",
    "\n",
    "<center>\n",
    "$$\n",
    "    MAE = \\frac{\\sum_{i}^N |\\hat{r}_i - r_i|}{N}\n",
    "$$\n",
    "</center>\n",
    "\n",
    "where $\\hat{r}_i$ are the predicted ratings and $r_i$ are the real ratings and $N$ is the number of items in the test set.\n",
    "\n",
    "**Task 1.** Implement MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(r_pred, r_real):\n",
    "    # Write your code here\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "# Very small differences\n",
    "print(\"MAE = {:.3f}\".format(mae(np.array([2.99, 1.98, 3.99, 4.97, 1.01]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Small differences\n",
    "print(\"MAE = {:.3f}\".format(mae(np.array([2.8, 1.7, 3.8, 4.6, 1.6]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Large differences\n",
    "print(\"MAE = {:.3f}\".format(mae(np.array([1.1, 4.2, 2.8, 3.2, 3.6]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Medium differences with one large difference\n",
    "print(\"MAE = {:.3f}\".format(mae(np.array([2.1, 1.2, 3.8, 4.2, 3.6]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Small differences with one very large difference\n",
    "print(\"MAE = {:.3f}\".format(mae(np.array([2.8, 1.7, 3.8, 4.6, 4.6]), np.array([3, 2, 4, 5, 1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-anderson",
   "metadata": {},
   "source": [
    "### RMSE - Root Mean Squared Error\n",
    "\n",
    "<center>\n",
    "$$\n",
    "    RMSE = \\sqrt{\\frac{\\sum_{i}^N (\\hat{r}_i - r_i)^2}{N}}\n",
    "$$\n",
    "</center>\n",
    "\n",
    "where $\\hat{r}_i$ are the predicted ratings and $r_i$ are the real ratings and $N$ is the number of items in the test set.\n",
    "\n",
    "**Task 2.** Implement RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(r_pred, r_real):\n",
    "    # Write your code here\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "# Very small differences\n",
    "print(\"RMSE = {:.3f}\".format(rmse(np.array([2.99, 1.98, 3.99, 4.97, 1.01]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Small differences\n",
    "print(\"RMSE = {:.3f}\".format(rmse(np.array([2.8, 1.7, 3.8, 4.6, 1.6]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Large differences\n",
    "print(\"RMSE = {:.3f}\".format(rmse(np.array([1.1, 4.2, 2.8, 3.2, 3.6]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Medium differences with one large difference\n",
    "print(\"RMSE = {:.3f}\".format(rmse(np.array([2.1, 1.2, 3.8, 4.2, 3.6]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Small differences with one very large difference\n",
    "print(\"RMSE = {:.3f}\".format(rmse(np.array([2.8, 1.7, 3.8, 4.6, 4.6]), np.array([3, 2, 4, 5, 1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-arrival",
   "metadata": {},
   "source": [
    "### MRE - Mean Relative Error\n",
    "\n",
    "<center>\n",
    "$$\n",
    "    MRE = \\frac{1}{N} \\sum_{i}^N \\frac{|\\hat{r}_i - r_i|}{|r_i|}\n",
    "$$\n",
    "</center>\n",
    "\n",
    "where $\\hat{r}_i$ are the predicted ratings and $r_i$ are the real ratings and $N$ is the number of items in the test set.\n",
    "\n",
    "**Task 3.** Implement MRE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mre(r_pred, r_real):\n",
    "    # Write your code here\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "# Very small differences\n",
    "print(\"MRE = {:.3f}\".format(mre(np.array([2.99, 1.98, 3.99, 4.97, 1.01]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Small differences\n",
    "print(\"MRE = {:.3f}\".format(mre(np.array([2.8, 1.7, 3.8, 4.6, 1.6]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Large differences\n",
    "print(\"MRE = {:.3f}\".format(mre(np.array([1.1, 4.2, 2.8, 3.2, 3.6]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Medium differences with one large difference\n",
    "print(\"MRE = {:.3f}\".format(mre(np.array([2.1, 1.2, 3.8, 4.2, 3.6]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Small differences with one very large difference for a small base value\n",
    "print(\"MRE = {:.3f}\".format(mre(np.array([2.8, 1.7, 3.8, 4.6, 4.6]), np.array([3, 2, 4, 5, 1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-contribution",
   "metadata": {},
   "source": [
    "### TRE - Total Relative Error\n",
    "\n",
    "<center>\n",
    "$$\n",
    "    TRE = \\frac{\\sum_{i}^N |\\hat{r}_i - r_i|}{\\sum_{i}^N |r_i|}\n",
    "$$\n",
    "</center>\n",
    "\n",
    "where $\\hat{r}_i$ are the predicted ratings and $r_i$ are the real ratings and $N$ is the number of items in the test set.\n",
    "\n",
    "**Task 4.** Implement TRE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tre(r_pred, r_real):\n",
    "    # Write your code here\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "# Very small differences\n",
    "print(\"TRE = {:.3f}\".format(tre(np.array([2.99, 1.98, 3.99, 4.97, 1.01]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Small differences\n",
    "print(\"TRE = {:.3f}\".format(tre(np.array([2.8, 1.7, 3.8, 4.6, 1.6]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Large differences\n",
    "print(\"TRE = {:.3f}\".format(tre(np.array([1.1, 4.2, 2.8, 3.2, 3.6]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Medium differences with one large difference\n",
    "print(\"TRE = {:.3f}\".format(tre(np.array([2.1, 1.2, 3.8, 4.2, 3.6]), np.array([3, 2, 4, 5, 1]))))\n",
    "# Small differences with one very large difference for a small base value\n",
    "print(\"TRE = {:.3f}\".format(tre(np.array([2.8, 1.7, 3.8, 4.6, 4.6]), np.array([3, 2, 4, 5, 1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-navigation",
   "metadata": {},
   "source": [
    "## Implicit feedback - binary indicators of interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-egypt",
   "metadata": {},
   "source": [
    "### HR@n - Hit Ratio \n",
    "How many hits did we score in the first n recommendations.\n",
    "<br/>\n",
    "<br/>\n",
    "<center>\n",
    "$$\n",
    "    \\text{HR@}n = \\frac{\\sum_{u} \\sum_{i \\in I_u} r_{u, i} \\cdot 1_{\\hat{D}_n(u)}(i)}{M}\n",
    "$$\n",
    "</center>\n",
    "\n",
    "where:\n",
    "  * $r_{u, i}$ is $1$ if there was an interaction between user $u$ and item $i$ in the test set and $0$ otherwise, \n",
    "  * $\\hat{D}_n$ is the set of the first $n$ recommendations for user $u$, \n",
    "  * $1_{\\hat{D}_n}(i)$ is $1$ if and only if $i \\in \\hat{D}_n$, otherwise it's equal to $0$,\n",
    "  * $M$ is the number of users.\n",
    "\n",
    "**Task 5.** Implement HR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr(recommendations, real_interactions, n=1):\n",
    "    \"\"\"\n",
    "    Assumes recommendations are ordered by user_id and then by score.\n",
    "    \"\"\"\n",
    "    # Write your code here\n",
    "    \n",
    "    \n",
    "    return hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1\n",
    "    \n",
    "real_interactions = pd.DataFrame(\n",
    "    [\n",
    "        [1, 45],\n",
    "        [1, 22],\n",
    "        [1, 77],\n",
    "        [2, 13],\n",
    "        [2, 77]\n",
    "        \n",
    "    ], columns=['user_id', 'item_id'])\n",
    "\n",
    "display(real_interactions)\n",
    "    \n",
    "recommendations = pd.DataFrame(\n",
    "    [\n",
    "        [1, 45, 0.9],\n",
    "        [1, 13, 0.8],\n",
    "        [1, 22, 0.71],\n",
    "        [1, 77, 0.55],\n",
    "        [1, 9, 0.52],\n",
    "        [2, 11, 0.85],\n",
    "        [2, 13, 0.69],\n",
    "        [2, 25, 0.64],\n",
    "        [2, 6, 0.60],\n",
    "        [2, 77, 0.53]\n",
    "        \n",
    "    ], columns=['user_id', 'item_id', 'score'])\n",
    "\n",
    "display(recommendations)\n",
    "    \n",
    "print(\"HR@3 = {:.4f}\".format(hr(recommendations, real_interactions, n=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2\n",
    "    \n",
    "real_interactions = pd.DataFrame(\n",
    "    [\n",
    "        [1, 45],\n",
    "        [1, 22],\n",
    "        [1, 77],\n",
    "        [2, 13],\n",
    "        [2, 77]\n",
    "        \n",
    "    ], columns=['user_id', 'item_id'])\n",
    "\n",
    "display(real_interactions)\n",
    "    \n",
    "recommendations = pd.DataFrame(\n",
    "    [\n",
    "        [1, 13, 0.9],\n",
    "        [1, 45, 0.8],\n",
    "        [1, 22, 0.71],\n",
    "        [1, 77, 0.55],\n",
    "        [1, 9, 0.52],\n",
    "        [2, 11, 0.85],\n",
    "        [2, 13, 0.69],\n",
    "        [2, 25, 0.64],\n",
    "        [2, 6, 0.60],\n",
    "        [2, 77, 0.53]\n",
    "        \n",
    "    ], columns=['user_id', 'item_id', 'score'])\n",
    "\n",
    "display(recommendations)\n",
    "    \n",
    "print(\"HR@3 = {:.4f}\".format(hr(recommendations, real_interactions, n=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-munich",
   "metadata": {},
   "source": [
    "### NDCG@n - Normalized Discounted Cumulative Gain\n",
    "\n",
    "How many hits did we score in the first n recommendations discounted by the position of each recommendation.\n",
    "<br/>\n",
    "<br/>\n",
    "<center>\n",
    "$$\n",
    "    \\text{NDCG@}n = \\frac{\\sum_{u} \\sum_{i \\in I_u} \\frac{r_{u, i}}{log\\left(1 + v_{\\hat{D}_n(u)}(i)\\right)}}{M}\n",
    "$$\n",
    "</center>\n",
    "\n",
    "where:\n",
    "  * $r_{u, i}$ is $1$ if there was an interaction between user $u$ and item $i$ in the test set and $0$ otherwise, \n",
    "  * $\\hat{D}_n(u)$ is the set of the first $n$ recommendations for user $u$, \n",
    "  * $v_{\\hat{D}_n(u)}(i)$ is the position of item $i$ in recommendations $\\hat{D}_n$,\n",
    "  * $M$ is the number of users.\n",
    "\n",
    "\n",
    "**Task 6.** Implement NDCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(recommendations, real_interactions, n=1):\n",
    "    \"\"\"\n",
    "    Assumes recommendations are ordered by user_id and then by score.\n",
    "    \"\"\"\n",
    "    # Write your code here\n",
    "    \n",
    "    \n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1\n",
    "    \n",
    "real_interactions = pd.DataFrame(\n",
    "    [\n",
    "        [1, 45],\n",
    "        [1, 22],\n",
    "        [1, 77],\n",
    "        [2, 13],\n",
    "        [2, 77]\n",
    "        \n",
    "    ], columns=['user_id', 'item_id'])\n",
    "\n",
    "display(real_interactions)\n",
    "    \n",
    "recommendations = pd.DataFrame(\n",
    "    [\n",
    "        [1, 45, 0.9],\n",
    "        [1, 13, 0.8],\n",
    "        [1, 22, 0.71],\n",
    "        [1, 77, 0.55],\n",
    "        [1, 9, 0.52],\n",
    "        [2, 11, 0.85],\n",
    "        [2, 13, 0.69],\n",
    "        [2, 25, 0.64],\n",
    "        [2, 6, 0.60],\n",
    "        [2, 77, 0.53]\n",
    "        \n",
    "    ], columns=['user_id', 'item_id', 'score'])\n",
    "\n",
    "display(recommendations)\n",
    "    \n",
    "print(\"NDCG@3 = {:.4f}\".format(ndcg(recommendations, real_interactions, n=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2\n",
    "    \n",
    "real_interactions = pd.DataFrame(\n",
    "    [\n",
    "        [1, 45],\n",
    "        [1, 22],\n",
    "        [1, 77],\n",
    "        [2, 13],\n",
    "        [2, 77]\n",
    "        \n",
    "    ], columns=['user_id', 'item_id'])\n",
    "\n",
    "display(real_interactions)\n",
    "    \n",
    "recommendations = pd.DataFrame(\n",
    "    [\n",
    "        [1, 13, 0.9],\n",
    "        [1, 45, 0.8],\n",
    "        [1, 22, 0.71],\n",
    "        [1, 77, 0.55],\n",
    "        [1, 9, 0.52],\n",
    "        [2, 11, 0.85],\n",
    "        [2, 13, 0.69],\n",
    "        [2, 25, 0.64],\n",
    "        [2, 6, 0.60],\n",
    "        [2, 77, 0.53]\n",
    "        \n",
    "    ], columns=['user_id', 'item_id', 'score'])\n",
    "\n",
    "display(recommendations)\n",
    "    \n",
    "print(\"NDCG@3 = {:.4f}\".format(ndcg(recommendations, real_interactions, n=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-baltimore",
   "metadata": {},
   "source": [
    "# Testing routines (offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-elevation",
   "metadata": {},
   "source": [
    "## Train and test set split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-blackjack",
   "metadata": {},
   "source": [
    "### Explicit feedback\n",
    "\n",
    "**Task 7.** Implement a method performing train-test split evaluation for explicit feedback for a given recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_train_test_split_explicit(recommender, interactions_df, items_df, seed=6789):\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    \n",
    "    if type(interactions_df) == dict:\n",
    "        # If interactions_df is a dict with already split data, use the split\n",
    "        interactions_df_train = interactions_df['train']\n",
    "        interactions_df_test = interactions_df['test']\n",
    "    else:    \n",
    "        # Otherwise split the dataset into train and test\n",
    "        # Write your code here\n",
    "       \n",
    "    \n",
    "    # Train the recommender\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    \n",
    "    # Gather predictions\n",
    "    \n",
    "    r_pred = []\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    \n",
    "    # Gather real ratings\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    \n",
    "    # Return evaluation metrics\n",
    "    \n",
    "    return rmse(r_pred, r_real), mre(r_pred, r_real), tre(r_pred, r_real)\n",
    "\n",
    "recommender = Recommender()\n",
    "\n",
    "results = [['BaseRecommender'] + list(evaluate_train_test_split_explicit(\n",
    "    recommender, ml_ratings_df.loc[:, ['user_id', 'item_id', 'rating']], ml_movies_df))]\n",
    "\n",
    "results = pd.DataFrame(results, \n",
    "                       columns=['Recommender', 'RMSE', 'MRE', 'TRE'])\n",
    "\n",
    "display(HTML(results.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-croatia",
   "metadata": {},
   "source": [
    "### Implicit feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-enclosure",
   "metadata": {},
   "source": [
    "**Task 8.** Implement a method performing train-test split evaluation for implicit feedback for a given recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_train_test_split_implicit(recommender, interactions_df, items_df, seed=6789):\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    \n",
    "    if type(interactions_df) == 'dict':\n",
    "        # If interactions_df is a dict with already split data, use the split\n",
    "        interactions_df_train = interactions_df['train']\n",
    "        interactions_df_test = interactions_df['test']\n",
    "    else:    \n",
    "        # Otherwise split the dataset into train and test\n",
    "\n",
    "        # Write your code here\n",
    "        \n",
    "    \n",
    "    hr_1 = []\n",
    "    hr_3 = []\n",
    "    hr_5 = []\n",
    "    hr_10 = []\n",
    "    ndcg_1 = []\n",
    "    ndcg_3 = []\n",
    "    ndcg_5 = []\n",
    "    ndcg_10 = []\n",
    "    \n",
    "    # Train the recommender\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    \n",
    "    # Make recommendations for each user in the test set and calculate the metric \n",
    "    # against all items of that user in the test set\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "                \n",
    "    hr_1 = np.mean(hr_1)\n",
    "    hr_3 = np.mean(hr_3)\n",
    "    hr_5 = np.mean(hr_5)\n",
    "    hr_10 = np.mean(hr_10)\n",
    "    ndcg_1 = np.mean(ndcg_1)\n",
    "    ndcg_3 = np.mean(ndcg_3)\n",
    "    ndcg_5 = np.mean(ndcg_5)\n",
    "    ndcg_10 = np.mean(ndcg_10)\n",
    "        \n",
    "    return hr_1, hr_3, hr_5, hr_10, ndcg_1, ndcg_3, ndcg_5, ndcg_10\n",
    "\n",
    "recommender = Recommender()\n",
    "\n",
    "results = [['BaseRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df))]\n",
    "\n",
    "results = pd.DataFrame(results, \n",
    "                       columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(results.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-tunisia",
   "metadata": {},
   "source": [
    "## Leave-one-out, leave-k-out, cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-spirit",
   "metadata": {},
   "source": [
    "### Explicit feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-luxembourg",
   "metadata": {},
   "source": [
    "**Task 9.** Implement a method performing leave one out evaluation for explicit feedback for a given recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_leave_one_out_explicit(recommender, interactions_df, items_df, max_evals=300, seed=6789):\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    \n",
    "    # Prepare splits of the datasets\n",
    "    # Write your code here\n",
    "    \n",
    "    \n",
    "    # For each split of the dataset train the recommender, generate recommendations and evaluate\n",
    "    \n",
    "    r_pred = []\n",
    "    r_real = []\n",
    "    n_eval = 1\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "        \n",
    "        if n_eval == max_evals:\n",
    "            break\n",
    "        n_eval += 1\n",
    "        \n",
    "    r_pred = np.array(r_pred)\n",
    "    r_real = np.array(r_real)\n",
    "        \n",
    "    # Return evaluation metrics\n",
    "    \n",
    "    return rmse(r_pred, r_real), mre(r_pred, r_real), tre(r_pred, r_real)\n",
    "\n",
    "recommender = Recommender()\n",
    "\n",
    "results = [['BaseRecommender'] + list(evaluate_leave_one_out_explicit(\n",
    "    recommender, ml_ratings_df.loc[:, ['user_id', 'item_id', 'rating']], ml_movies_df))]\n",
    "\n",
    "results = pd.DataFrame(results, \n",
    "                       columns=['Recommender', 'RMSE', 'MRE', 'TRE'])\n",
    "\n",
    "display(HTML(results.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-lloyd",
   "metadata": {},
   "source": [
    "### Implicit feedback\n",
    "\n",
    "**Task 10.** Implement a method performing leave one out evaluation for implicit feedback for a given recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_leave_one_out_implicit(recommender, interactions_df, items_df, max_evals=300, seed=6789):\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    \n",
    "    # Prepare splits of the datasets\n",
    "    # Write your code here\n",
    "    \n",
    "    \n",
    "    hr_1 = []\n",
    "    hr_3 = []\n",
    "    hr_5 = []\n",
    "    hr_10 = []\n",
    "    ndcg_1 = []\n",
    "    ndcg_3 = []\n",
    "    ndcg_5 = []\n",
    "    ndcg_10 = []\n",
    "    \n",
    "    # For each split of the dataset train the recommender, generate recommendations and evaluate\n",
    "    \n",
    "    n_eval = 1\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "        \n",
    "        if n_eval == max_evals:\n",
    "            break\n",
    "        n_eval += 1\n",
    "        \n",
    "    hr_1 = np.mean(hr_1)\n",
    "    hr_3 = np.mean(hr_3)\n",
    "    hr_5 = np.mean(hr_5)\n",
    "    hr_10 = np.mean(hr_10)\n",
    "    ndcg_1 = np.mean(ndcg_1)\n",
    "    ndcg_3 = np.mean(ndcg_3)\n",
    "    ndcg_5 = np.mean(ndcg_5)\n",
    "    ndcg_10 = np.mean(ndcg_10)\n",
    "    \n",
    "    return hr_1, hr_3, hr_5, hr_10, ndcg_1, ndcg_3, ndcg_5, ndcg_10\n",
    "\n",
    "recommender = Recommender()\n",
    "\n",
    "results = [['BaseRecommender'] + list(evaluate_leave_one_out_implicit(\n",
    "    recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df))]\n",
    "\n",
    "results = pd.DataFrame(results, \n",
    "                       columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(results.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-chain",
   "metadata": {},
   "source": [
    "# Examples of evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-haven",
   "metadata": {},
   "source": [
    "## Explicit feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-archives",
   "metadata": {},
   "source": [
    "### Train-test split test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.basic_recommenders import MostPopularRecommender\n",
    "from recommenders.basic_recommenders import HighestRatedRecommender\n",
    "from recommenders.basic_content_based_recommenders import LinearRegressionRecommender\n",
    "from recommenders.basic_content_based_recommenders import SVRRecommender\n",
    "from recommenders.nearest_neighbors_recommender import ItemBasedCosineNearestNeighborsRecommender\n",
    "\n",
    "highest_rated_recommender = HighestRatedRecommender()\n",
    "lr_recommender = LinearRegressionRecommender()\n",
    "svr_recommender = SVRRecommender()\n",
    "\n",
    "recommenders = [highest_rated_recommender, lr_recommender, svr_recommender]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for recommender in recommenders:\n",
    "    results = [[type(recommender).__name__] + list(evaluate_train_test_split_explicit(\n",
    "        recommender, ml_ratings_df, ml_movies_df))]\n",
    "\n",
    "    results = pd.DataFrame(results, \n",
    "                       columns=['Recommender', 'RMSE', 'MRE', 'TRE'])\n",
    "    all_results.append(results)\n",
    "\n",
    "    display(results)\n",
    "    \n",
    "all_results = pd.concat(all_results).reset_index(drop=True)\n",
    "display(all_results)\n",
    "\n",
    "print('Total evaluation time: {}'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-warehouse",
   "metadata": {},
   "source": [
    "### Leave-one-out test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.basic_recommenders import MostPopularRecommender\n",
    "from recommenders.basic_recommenders import HighestRatedRecommender\n",
    "from recommenders.basic_content_based_recommenders import LinearRegressionRecommender\n",
    "from recommenders.basic_content_based_recommenders import SVRRecommender\n",
    "from recommenders.nearest_neighbors_recommender import ItemBasedCosineNearestNeighborsRecommender\n",
    "\n",
    "highest_rated_recommender = HighestRatedRecommender()\n",
    "lr_recommender = LinearRegressionRecommender()\n",
    "svr_recommender = SVRRecommender()\n",
    "\n",
    "recommenders = [highest_rated_recommender, lr_recommender, svr_recommender]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for recommender in recommenders:\n",
    "    results = [[type(recommender).__name__] + list(evaluate_leave_one_out_explicit(\n",
    "        recommender, ml_ratings_df, ml_movies_df))]\n",
    "\n",
    "    results = pd.DataFrame(results, \n",
    "                       columns=['Recommender', 'RMSE', 'MRE', 'TRE'])\n",
    "    all_results.append(results)\n",
    "\n",
    "    display(results)\n",
    "    \n",
    "all_results = pd.concat(all_results).reset_index(drop=True)\n",
    "display(all_results)\n",
    "\n",
    "print('Total evaluation time: {}'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-purchase",
   "metadata": {},
   "source": [
    "## Implicit feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-liberty",
   "metadata": {},
   "source": [
    "### Train-test split test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.basic_recommenders import MostPopularRecommender\n",
    "from recommenders.basic_recommenders import HighestRatedRecommender\n",
    "from recommenders.basic_content_based_recommenders import LinearRegressionRecommender\n",
    "from recommenders.basic_content_based_recommenders import SVRRecommender\n",
    "from recommenders.nearest_neighbors_recommender import ItemBasedCosineNearestNeighborsRecommender\n",
    "\n",
    "most_popular_recommender = MostPopularRecommender()\n",
    "highest_rated_recommender = HighestRatedRecommender()\n",
    "lr_recommender = LinearRegressionRecommender()\n",
    "svr_recommender = SVRRecommender()\n",
    "ibcnn_recommender = ItemBasedCosineNearestNeighborsRecommender(n_neighbors=30)\n",
    "\n",
    "recommenders = [most_popular_recommender, highest_rated_recommender, lr_recommender, svr_recommender, ibcnn_recommender]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for recommender in recommenders:\n",
    "    results = [[type(recommender).__name__] + list(evaluate_train_test_split_implicit(\n",
    "        recommender, ml_ratings_df, ml_movies_df))]\n",
    "\n",
    "    results = pd.DataFrame(results, \n",
    "                           columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "    all_results.append(results)\n",
    "\n",
    "    display(results)\n",
    "    \n",
    "all_results = pd.concat(all_results).reset_index(drop=True)\n",
    "display(all_results)\n",
    "\n",
    "print('Total evaluation time: {}'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-beast",
   "metadata": {},
   "source": [
    "### Leave-one-out test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.basic_recommenders import MostPopularRecommender\n",
    "from recommenders.basic_recommenders import HighestRatedRecommender\n",
    "from recommenders.basic_content_based_recommenders import LinearRegressionRecommender\n",
    "from recommenders.basic_content_based_recommenders import SVRRecommender\n",
    "from recommenders.nearest_neighbors_recommender import ItemBasedCosineNearestNeighborsRecommender\n",
    "\n",
    "most_popular_recommender = MostPopularRecommender()\n",
    "highest_rated_recommender = HighestRatedRecommender()\n",
    "lr_recommender = LinearRegressionRecommender()\n",
    "svr_recommender = SVRRecommender()\n",
    "ibcnn_recommender = ItemBasedCosineNearestNeighborsRecommender(n_neighbors=30)\n",
    "\n",
    "recommenders = [most_popular_recommender, highest_rated_recommender, lr_recommender, svr_recommender, ibcnn_recommender]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for recommender in recommenders:\n",
    "    results = [[type(recommender).__name__] + list(evaluate_leave_one_out_implicit(\n",
    "        recommender, ml_ratings_df, ml_movies_df))]\n",
    "\n",
    "    results = pd.DataFrame(results, \n",
    "                           columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "    all_results.append(results)\n",
    "\n",
    "    display(results)\n",
    "    \n",
    "all_results = pd.concat(all_results).reset_index(drop=True)\n",
    "display(all_results)\n",
    "\n",
    "print('Total evaluation time: {}'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-insertion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
